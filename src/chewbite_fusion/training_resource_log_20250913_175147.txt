[2025-09-13 17:51:48] ============================================================
[2025-09-13 17:51:48] 【训练数据信息】
[2025-09-13 17:51:48] 原始X类型: <class 'list'>, 长度: 1
[2025-09-13 17:51:48] 原始y类型: <class 'list'>, 长度: 42
[2025-09-13 17:51:48] ============================================================
[2025-09-13 17:51:48] 
===== 转换样本为NumPy数组 =====
[2025-09-13 17:51:48] 样本0：已从list转换为数组，形状=(288, 1800, 1)，原始长度=288
[2025-09-13 17:51:48] 样本1：已从list转换为数组，形状=(227, 1800, 1)，原始长度=227
[2025-09-13 17:51:48] 样本2：已从list转换为数组，形状=(381, 1800, 1)，原始长度=381
[2025-09-13 17:51:48] 样本3：已从list转换为数组，形状=(426, 1800, 1)，原始长度=426
[2025-09-13 17:51:48] 样本4：已从list转换为数组，形状=(406, 1800, 1)，原始长度=406
[2025-09-13 17:51:48] 样本5：已从list转换为数组，形状=(516, 1800, 1)，原始长度=516
[2025-09-13 17:51:48] 样本6：已从list转换为数组，形状=(125, 1800, 1)，原始长度=125
[2025-09-13 17:51:48] 样本7：已从list转换为数组，形状=(324, 1800, 1)，原始长度=324
[2025-09-13 17:51:48] 样本8：已从list转换为数组，形状=(202, 1800, 1)，原始长度=202
[2025-09-13 17:51:48] 样本9：已从list转换为数组，形状=(137, 1800, 1)，原始长度=137
[2025-09-13 17:51:48] 样本10：已从list转换为数组，形状=(276, 1800, 1)，原始长度=276
[2025-09-13 17:51:48] 样本11：已从list转换为数组，形状=(417, 1800, 1)，原始长度=417
[2025-09-13 17:51:48] 样本12：已从list转换为数组，形状=(455, 1800, 1)，原始长度=455
[2025-09-13 17:51:48] 样本13：已从list转换为数组，形状=(446, 1800, 1)，原始长度=446
[2025-09-13 17:51:48] 样本14：已从list转换为数组，形状=(518, 1800, 1)，原始长度=518
[2025-09-13 17:51:48] 样本15：已从list转换为数组，形状=(835, 1800, 1)，原始长度=835
[2025-09-13 17:51:48] 样本16：已从list转换为数组，形状=(144, 1800, 1)，原始长度=144
[2025-09-13 17:51:48] 样本17：已从list转换为数组，形状=(1013, 1800, 1)，原始长度=1013
[2025-09-13 17:51:48] 样本18：已从list转换为数组，形状=(238, 1800, 1)，原始长度=238
[2025-09-13 17:51:48] 样本19：已从list转换为数组，形状=(714, 1800, 1)，原始长度=714
[2025-09-13 17:51:48] 样本20：已从list转换为数组，形状=(475, 1800, 1)，原始长度=475
[2025-09-13 17:51:48] 样本21：已从list转换为数组，形状=(301, 1800, 1)，原始长度=301
[2025-09-13 17:51:48] 样本22：已从list转换为数组，形状=(351, 1800, 1)，原始长度=351
[2025-09-13 17:51:48] 样本23：已从list转换为数组，形状=(635, 1800, 1)，原始长度=635
[2025-09-13 17:51:48] 样本24：已从list转换为数组，形状=(337, 1800, 1)，原始长度=337
[2025-09-13 17:51:48] 样本25：已从list转换为数组，形状=(537, 1800, 1)，原始长度=537
[2025-09-13 17:51:48] 样本26：已从list转换为数组，形状=(347, 1800, 1)，原始长度=347
[2025-09-13 17:51:48] 样本27：已从list转换为数组，形状=(355, 1800, 1)，原始长度=355
[2025-09-13 17:51:48] 样本28：已从list转换为数组，形状=(192, 1800, 1)，原始长度=192
[2025-09-13 17:51:48] 样本29：已从list转换为数组，形状=(386, 1800, 1)，原始长度=386
[2025-09-13 17:51:48] 样本30：已从list转换为数组，形状=(784, 1800, 1)，原始长度=784
[2025-09-13 17:51:48] 样本31：已从list转换为数组，形状=(479, 1800, 1)，原始长度=479
[2025-09-13 17:51:48] 样本32：已从list转换为数组，形状=(390, 1800, 1)，原始长度=390
[2025-09-13 17:51:48] 样本33：已从list转换为数组，形状=(551, 1800, 1)，原始长度=551
[2025-09-13 17:51:48] 样本34：已从list转换为数组，形状=(271, 1800, 1)，原始长度=271
[2025-09-13 17:51:48] 样本35：已从list转换为数组，形状=(446, 1800, 1)，原始长度=446
[2025-09-13 17:51:48] 样本36：已从list转换为数组，形状=(539, 1800, 1)，原始长度=539
[2025-09-13 17:51:48] 样本37：已从list转换为数组，形状=(443, 1800, 1)，原始长度=443
[2025-09-13 17:51:48] 样本38：已从list转换为数组，形状=(201, 1800, 1)，原始长度=201
[2025-09-13 17:51:48] 样本39：已从list转换为数组，形状=(294, 1800, 1)，原始长度=294
[2025-09-13 17:51:48] 样本40：已从list转换为数组，形状=(230, 1800, 1)，原始长度=230
[2025-09-13 17:51:48] 样本41：已从list转换为数组，形状=(810, 1800, 1)，原始长度=810
[2025-09-13 17:51:48] ===========================

[2025-09-13 17:51:48] 当前批次最长序列长度（窗口数）: 1013
[2025-09-13 17:51:49] X填充后形状: (42, 1013, 1800, 1)
[2025-09-13 17:51:49] 检测到的类别: [0, 1, 2, 3], 填充类别编号: 4
[2025-09-13 17:51:49] y填充后形状: (42, 1013)
[2025-09-13 17:51:49] X填充值替换为均值: -0.0002
[2025-09-13 17:51:49] 标准化后X统计: min=-19.2088, max=19.3730
[2025-09-13 17:51:49] 已检测到 3 个GPU，将用于分布式训练
[2025-09-13 17:51:51] 
模型初始化完成（多GPU支持），结构如下：
[2025-09-13 17:51:51] Model: "sequential"
[2025-09-13 17:51:51] _________________________________________________________________
[2025-09-13 17:51:51]  Layer (type)                Output Shape              Param #   
[2025-09-13 17:51:51] =================================================================
[2025-09-13 17:51:51]  time_distributed_cnn (TimeD  (None, 1013, 3200)       57152     
[2025-09-13 17:51:51]  istributed)                                                     
[2025-09-13 17:51:51]                                                                  
[2025-09-13 17:51:51]  bidirectional_gru (Bidirect  (None, 1013, 256)        2557440   
[2025-09-13 17:51:51]  ional)                                                          
[2025-09-13 17:51:51]                                                                  
[2025-09-13 17:51:51]  time_distributed_ffn (TimeD  (None, 1013, 5)          100869    
[2025-09-13 17:51:51]  istributed)                                                     
[2025-09-13 17:51:51]                                                                  
[2025-09-13 17:51:51] =================================================================
[2025-09-13 17:51:51] Total params: 2,715,461
[2025-09-13 17:51:51] Trainable params: 2,714,181
[2025-09-13 17:51:51] Non-trainable params: 1,280
[2025-09-13 17:51:51] _________________________________________________________________
[2025-09-13 17:51:51] 监控批次形状 - X: (5, 1013, 1800, 1), y: (5, 1013)
[2025-09-13 17:51:51] 使用验证集: True, 验证比例: 0.2
[2025-09-13 17:51:51] 
===== 类别权重 =====
[2025-09-13 17:51:51] 类别 0 (普通类别): 样本数=1769, 权重=2.7207
[2025-09-13 17:51:51] 类别 1 (普通类别): 样本数=6348, 权重=1.6003
[2025-09-13 17:51:51] 类别 2 (普通类别): 样本数=3399, 权重=2.1265
[2025-09-13 17:51:51] 类别 3 (普通类别): 样本数=5926, 权重=1.6556
[2025-09-13 17:51:51] 类别 4 (填充类别): 样本数=25104, 权重=0.0000
[2025-09-13 17:51:51] ====================

[2025-09-13 17:51:51] 样本权重范围: [0.0000, 2.7207]
[2025-09-13 17:51:51] 
【开始训练】样本数: 42, 批次大小: 5, GPU数量: 3
[2025-09-13 17:51:52] 
===== 系统资源监控初始化 =====
[2025-09-13 17:51:52] ===== 系统基本信息 =====
[2025-09-13 17:51:52] CPU核心数: 48 (物理核心: 24)
[2025-09-13 17:51:52] 总内存: 251.56 GB
[2025-09-13 17:51:52] 初始可用内存: 192.88 GB
[2025-09-13 17:51:52] GPU 0: NVIDIA GeForce RTX 3090
[2025-09-13 17:51:52]   总显存: 24.00 GB
[2025-09-13 17:51:52]   初始可用显存: 0.79 GB
[2025-09-13 17:51:52] GPU 1: NVIDIA GeForce RTX 3090
[2025-09-13 17:51:52]   总显存: 24.00 GB
[2025-09-13 17:51:52]   初始可用显存: 4.82 GB
[2025-09-13 17:51:52] GPU 2: NVIDIA GeForce RTX 3090
[2025-09-13 17:51:52]   总显存: 24.00 GB
[2025-09-13 17:51:52]   初始可用显存: 4.82 GB
[2025-09-13 17:51:52] ========================

[2025-09-13 17:53:43] 
===== Epoch 0 层 time_distributed_cnn 输出监控 =====
[2025-09-13 17:53:43] 形状: (5, 1013, 3200)
[2025-09-13 17:53:43] 最小值: 0.000000, 最大值: 47.915806
[2025-09-13 17:53:43] 含Inf: False
[2025-09-13 17:53:43] =========================================

[2025-09-13 17:53:43] 
===== Epoch 0 层 bidirectional_gru 输出监控 =====
[2025-09-13 17:53:43] 形状: (5, 1013, 256)
[2025-09-13 17:53:43] 最小值: -1.000000, 最大值: 1.000000
[2025-09-13 17:53:43] 含Inf: False
[2025-09-13 17:53:43] =========================================

[2025-09-13 17:53:43] 
===== Epoch 0 层 time_distributed_ffn 输出监控 =====
[2025-09-13 17:53:43] 形状: (5, 1013, 5)
[2025-09-13 17:53:43] 最小值: 0.012006, 最大值: 0.848586
[2025-09-13 17:53:43] 含Inf: False
[2025-09-13 17:53:43] =========================================

[2025-09-13 17:54:11] 梯度监控出错: Exception encountered when calling layer 'gru_cell_2' (type GRUCell).

{{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[2,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MatMul]

Call arguments received by layer 'gru_cell_2' (type GRUCell):
  • inputs=tf.Tensor(shape=(2, 3200), dtype=float32)
  • states=('tf.Tensor(shape=(2, 128), dtype=float32)',)
  • training=True
[2025-09-13 17:54:11] 
===== Epoch 0 资源使用统计 =====
[2025-09-13 17:54:11] CPU使用率: 0.4%
[2025-09-13 17:54:11] 内存状态: 已用 56.54 GB / 总 251.56 GB / 可用 191.81 GB (76.2%)
[2025-09-13 17:54:11] GPU 0 状态: 使用率 0%, 显存(已用/总/可用): 23.21 / 24.00 / 0.79 GB, 温度 45°C
[2025-09-13 17:54:11] GPU 1 状态: 使用率 0%, 显存(已用/总/可用): 19.18 / 24.00 / 4.82 GB, 温度 45°C
[2025-09-13 17:54:11] GPU 2 状态: 使用率 0%, 显存(已用/总/可用): 19.18 / 24.00 / 4.82 GB, 温度 37°C
[2025-09-13 17:54:11] 当前进程内存使用: 11.25 GB
[2025-09-13 17:54:11] =================================

[2025-09-13 17:55:16] 训练过程出错: Graph execution error:

Detected at node 'gradient_tape/sequential/bidirectional_gru/backward_gru/while/zeros_1' defined at (most recent call last):
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 973, in _bootstrap
      self._bootstrap_inner()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
      self.run()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1249, in run_step
      outputs = model.train_step(data)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1054, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 542, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 275, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'gradient_tape/sequential/bidirectional_gru/backward_gru/while/zeros_1'
Detected at node 'gradient_tape/sequential/bidirectional_gru/backward_gru/while/zeros_1' defined at (most recent call last):
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 973, in _bootstrap
      self._bootstrap_inner()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
      self.run()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1249, in run_step
      outputs = model.train_step(data)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1054, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 542, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 275, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'gradient_tape/sequential/bidirectional_gru/backward_gru/while/zeros_1'
Detected at node 'gradient_tape/sequential/bidirectional_gru/backward_gru/while/zeros_1' defined at (most recent call last):
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 973, in _bootstrap
      self._bootstrap_inner()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
      self.run()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1249, in run_step
      outputs = model.train_step(data)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1054, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 542, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 275, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'gradient_tape/sequential/bidirectional_gru/backward_gru/while/zeros_1'
Detected at node 'gradient_tape/sequential/bidirectional_gru/backward_gru/while/zeros_1' defined at (most recent call last):
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 973, in _bootstrap
      self._bootstrap_inner()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
      self.run()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1249, in run_step
      outputs = model.train_step(data)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1054, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 542, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 275, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'gradient_tape/sequential/bidirectional_gru/backward_gru/while/zeros_1'
4 root error(s) found.
  (0) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[3200,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node gradient_tape/sequential/bidirectional_gru/backward_gru/while/zeros_1}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert/_919]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[div_no_nan_1/ReadVariableOp_2/_1052]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[group_deps/_1113]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

  (1) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[3200,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node gradient_tape/sequential/bidirectional_gru/backward_gru/while/zeros_1}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert/_919]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[div_no_nan_1/ReadVariableOp_2/_1052]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

  (2) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[3200,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node gradient_tape/sequential/bidirectional_gru/backward_gru/while/zeros_1}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert/_919]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

  (3) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[3200,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node gradient_tape/sequential/bidirectional_gru/backward_gru/while/zeros_1}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_2290932]